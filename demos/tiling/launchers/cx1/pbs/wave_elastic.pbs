#!/bin/bash

# Note: mpiexec.1pps runs hybrid mpi+openmp jobs
# Behind the scenes, it sets:
# I_MPI_PIN=yes
# I_MPI_PIN_MODE=lib
# I_MPI_PIN_DOMAIN=socket
# I_MPI_PIN_ORDER=compact
# KMP_AFFINITY=granularity=fine,compact,1,0

FIREDRAKE=$FIREDRAKE_DIR
TILING=$FIREDRAKE/demos/tiling
EXECUTABLE=$TILING/wave_elastic.py
MESHES=$WORK/meshes/wave_elastic


echo ------------------------------------------------------
echo -n 'Job is running on node '; cat $PBS_NODEFILE
cat /proc/cpuinfo | grep "model name" | uniq
echo ------------------------------------------------------
echo PBS: qsub is running on $PBS_O_HOST
echo PBS: originating queue is $PBS_O_QUEUE
echo PBS: executing queue is $PBS_QUEUE
echo PBS: working directory is $PBS_O_WORKDIR
echo PBS: execution mode is $PBS_ENVIRONMENT
echo PBS: job identifier is $PBS_JOBID
echo PBS: job name is $PBS_JOBNAME
echo PBS: node file is $PBS_NODEFILE
echo PBS: current home directory is $PBS_O_HOME
echo PBS: PATH = $PBS_O_PATH
echo ------------------------------------------------------
echo PBS: PYTHONPATH = $PYTHONPATH
echo ------------------------------------------------------
echo PBS: SLOPE_BACKEND = $SLOPE_BACKEND
echo ------------------------------------------------------


export OMP_NUM_THREADS=1
export SLOPE_BACKEND=SEQUENTIAL

OPTS="--output 100 --flatten True"
TILE_OPTS="--extra-halo 1 --fusion-mode only_tile"

LOGGER=$WORK"/logger.txt"
rm -f $LOGGER
touch $LOGGER

# Clean the remote cache.
# Note: the first run is on a tiny mesh; to be interpreted as a ``warm-up'' run, to populate the cache
$FIREDRAKE/scripts/firedrake-clean

for poly in 1 2 3
do
    OUT_FILE=$WORK"/output_p"$poly".txt"
    echo "Polynomial order "$poly >> $LOGGER
    for MESH in "--mesh-size (50.0,25.0,2.5)" "--mesh-size (300.0,150.0,0.4)"
    do
        rm -f $OUT_FILE
        touch $OUT_FILE
        echo "    Running "$MESH >> $LOGGER
        for p in "metis"
        do
            echo "        Untiled ..." >> $LOGGER
            mpiexec python $EXECUTABLE --poly-order $poly $MESH $OPTS --num-unroll 0 --part-mode $p 1>> $OUT_FILE 2>> $OUT_FILE
            for sm in 4
            do
                for ts in 40 70 150 300 1000
                do
                    echo "        Tiled (pm="$p", ts="$ts") ..." >> $LOGGER
                    mpiexec python $EXECUTABLE --poly-order $poly $MESH $OPTS --num-unroll 1 --tile-size $ts --part-mode $p --split-mode $sm $TILE_OPTS 1>> $OUT_FILE 2>> $OUT_FILE
                done
            done
        done
    done
done

rm $LOGGER

# The pure OMP version needs affinity explicitly set
export KMP_AFFINITY=granularity=fine,compact,1,0
export OMP_NUM_THREADS=20
export SLOPE_BACKEND=OMP
# No OMP experiments
