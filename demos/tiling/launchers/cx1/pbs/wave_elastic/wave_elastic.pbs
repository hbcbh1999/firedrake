#!/bin/bash

# Note: mpiexec.1pps runs hybrid mpi+openmp jobs
# Behind the scenes, it sets:
# I_MPI_PIN=yes
# I_MPI_PIN_MODE=lib
# I_MPI_PIN_DOMAIN=socket
# I_MPI_PIN_ORDER=compact
# KMP_AFFINITY=granularity=fine,compact,1,0

FIREDRAKE=$FIREDRAKE_DIR
TILING=$FIREDRAKE/demos/tiling
EXECUTABLE=$TILING/wave_elastic.py
MESHES=$WORK/meshes/wave_elastic


echo ------------------------------------------------------
echo -n 'Job is running on node '; cat $PBS_NODEFILE
cat /proc/cpuinfo | grep "model name" | uniq
echo ------------------------------------------------------
echo PBS: qsub is running on $PBS_O_HOST
echo PBS: originating queue is $PBS_O_QUEUE
echo PBS: executing queue is $PBS_QUEUE
echo PBS: working directory is $PBS_O_WORKDIR
echo PBS: execution mode is $PBS_ENVIRONMENT
echo PBS: job identifier is $PBS_JOBID
echo PBS: job name is $PBS_JOBNAME
echo PBS: node file is $PBS_NODEFILE
echo PBS: current home directory is $PBS_O_HOME
echo PBS: PATH = $PBS_O_PATH
echo ------------------------------------------------------
echo PBS: PYTHONPATH = $PYTHONPATH
echo ------------------------------------------------------
echo PBS: SLOPE_BACKEND = $SLOPE_BACKEND
echo ------------------------------------------------------


export OMP_NUM_THREADS=1
export SLOPE_BACKEND=SEQUENTIAL

OPTS="--output 10000 --flatten True --nocache True"
TILE_OPTS="--extra-halo 1 --fusion-mode only_tile --glb-maps True --coloring default"

LOGGER=$WORK"/logger_"$PBS_JOBNAME".txt"
rm -f $LOGGER
touch $LOGGER

# Clean the remote cache.
# Note: the first run is on a tiny mesh; to be interpreted as a ``warm-up'' run, to populate the cache
$FIREDRAKE/scripts/firedrake-clean

# Tile sizes for each poly order
declare -a ts_p1=(150 230 310 400)
declare -a ts_p2=(70 140 200 300)
declare -a ts_p3=(40 70 100)
declare -a ts_p4=(30 60 90)

# Partition modes for each poly order
declare -a part_p1=("chunk")
declare -a part_p2=("chunk")
declare -a part_p3=("chunk" "metis")
declare -a part_p4=("metis")

for poly in 1 2 3 4
do
    OUT_FILE=$WORK"/output_"$PBS_JOBNAME"_p"$poly".txt"
    rm -f $OUT_FILE
    touch $OUT_FILE
    echo "Polynomial order "$poly >> $LOGGER
    for MESH in "--mesh-size (300.0,150.0,0.6)" "--mesh-file "$MESHES"/domain_h060.msh --h 0.60"
    do
        echo "    Running "$MESH >> $LOGGER
        echo "        Untiled ..." >> $LOGGER
        mpiexec python $EXECUTABLE --poly-order $poly $MESH $OPTS --num-unroll 0 1>> $OUT_FILE 2>> $OUT_FILE
        mpiexec python $EXECUTABLE --poly-order $poly $MESH $OPTS --num-unroll 0 1>> $OUT_FILE 2>> $OUT_FILE
        part_p="part_p$poly[*]"
        for p in ${!part_p}
        do
            for em in 2 3 4 5
            do
                ts_p="ts_p$poly[*]"
                for ts in ${!ts_p}
                do
                    echo "        Tiled (pm="$p", ts="$ts", em="$em") ..."
                    mpiexec python $EXECUTABLE --poly-order $poly $MESH $OPTS --num-unroll 1 --tile-size $ts --part-mode $p --explicit-mode $em $TILE_OPTS 1>> $OUT_FILE 2>> $OUT_FILE
                done
            done
        done
    done
done

rm $LOGGER

# The pure OMP version needs affinity explicitly set
export KMP_AFFINITY=granularity=fine,compact,1,0
export OMP_NUM_THREADS=20
export SLOPE_BACKEND=OMP
# No OMP experiments
